{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7217d9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.chains import create_history_aware_retriever,create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import requests\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af09a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OllamaEmbeddings(model=\"llama2:7b\")\n",
    "api_key=os.getenv(\"GROQ_API\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=api_key,model_name='Llama3-8b-8192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74a1785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF to ./storage/title.pdf\n"
     ]
    }
   ],
   "source": [
    "name=\"title.pdf\"\n",
    "def download_pdf(url, output_path=f\"./storage/{name}\"):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded PDF to {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download PDF: {response.status_code}\")\n",
    "\n",
    "pdf_path = download_pdf(\"https://www.dropbox.com/scl/fi/tqc5bz8ete0j8n6ihbntr/AI_ML-SD-Intern.pdf?rlkey=netrg0jdt1jan7j9ggief6tl0&dl=1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239d28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./temp.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651253a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukka\\AppData\\Local\\Temp\\ipykernel_18992\\1618585710.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "persist_directory = './testing_db'\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae6fae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "contextualized_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualized_q_prompt)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant that answers questions based on the provided context. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"You can also ask for clarification if needed. Answer concisely.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd9ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: According to the job description, the stipend for the Software Developer Intern position is up to 50,000 per month.\n",
      "\n",
      "--- Chat History ---\n",
      "You: i want to know the stipend\n",
      "Assistant: According to the job description, the stipend for the Software Developer Intern position is up to 50,000 per month.\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# 6. Session and Message History\n",
    "session_id = \"default_session\"\n",
    "store = {}\n",
    "def get_session_history(session: str) -> BaseChatMessageHistory:\n",
    "    if session not in store:\n",
    "        store[session] = ChatMessageHistory()\n",
    "    return store[session]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "\n",
    "# 7. Interactive Chat Loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    session_history = get_session_history(session_id)\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"Assistant:\", response[\"answer\"])\n",
    "    print(\"\\n--- Chat History ---\")\n",
    "    for msg in session_history.messages:\n",
    "        role = \"You\" if msg.type == \"human\" else \"Assistant\"\n",
    "        print(f\"{role}: {msg.content}\")\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207c604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
